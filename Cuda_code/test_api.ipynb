{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc2hdd/home/ysi538/miniconda3/envs/yhre-new/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' # 请求数据\\n# url = \\'  /hpc/retrieval/query/image2image_retrieval\\'\\nurl = \\'http://localhost:9876/test\\'\\nquery_img_path = \" metaservice/api/region/openslide/241183-21.tiff/6400/25344/256/256/1\"\\nrequest_data = {\\n    \\'query_img_path\\': query_img_path,\\n    \\'top_k\\': 20\\n}\\n\\n# 发送 POST 请求\\nresponse = requests.post(url, json=request_data)\\n\\n# 检查响应状态码\\nif response.status_code == 200:\\n    response_data = response.json()\\n    print(\"Response JSON:\", response_data)\\nelse:\\n    print(\"Request failed with status code:\", response.status_code) '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import numpy as np \n",
    "import cupy as cp\n",
    "import sys\n",
    "from cuvs.neighbors import hnsw,cagra\n",
    "import os\n",
    "import time\n",
    "sys.path.append('/hpc2hdd/home/ysi538/retrieval')\n",
    "from MDI_RAG_Image2Image_Research.src.utils.encoder import WSI_Image_UNI_Encoder\n",
    "\"\"\" # 请求数据\n",
    "# url = '  /hpc/retrieval/query/image2image_retrieval'\n",
    "url = 'http://localhost:9876/test'\n",
    "query_img_path = \" metaservice/api/region/openslide/241183-21.tiff/6400/25344/256/256/1\"\n",
    "request_data = {\n",
    "    'query_img_path': query_img_path,\n",
    "    'top_k': 20\n",
    "}\n",
    "\n",
    "# 发送 POST 请求\n",
    "response = requests.post(url, json=request_data)\n",
    "\n",
    "# 检查响应状态码\n",
    "if response.status_code == 200:\n",
    "    response_data = response.json()\n",
    "    print(\"Response JSON:\", response_data)\n",
    "else:\n",
    "    print(\"Request failed with status code:\", response.status_code) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_regions(result_infos):\n",
    "\n",
    "    search_info_list = []\n",
    "    \n",
    "    for info in result_infos:\n",
    "        search_info = {}\n",
    "        level = info.split(\"_\")[-1].split(\".\")[0]\n",
    "        w = info.split(\"_\")[-3]\n",
    "        h = info.split(\"_\")[-2]\n",
    "        x = info.split(\"_\")[-5]\n",
    "        y = info.split(\"_\")[-4]\n",
    "        id = info.split(\"_\")[0]\n",
    "        name = \"_\".join(info.split(\"_\")[1:-5])\n",
    "        search_info = {\"id\": id, \"name\": name, \"x\": x, \"y\": y, \"w\": w, \"h\": h, \"level\": level}\n",
    "        search_info_list.append(search_info)\n",
    "\n",
    "    def dfs(node, component, visited):\n",
    "        # 将当前节点标记为已访问\n",
    "        visited[node] = True\n",
    "        # 将当前节点添加到当前组件\n",
    "        component.append(search_info_list[node])\n",
    "        \n",
    "        # 遍历其他所有节点\n",
    "        for neighbor in range(len(search_info_list)):\n",
    "            if not visited[neighbor] and judge_if_connected(search_info_list[node], search_info_list[neighbor]):\n",
    "                # 如果邻居节点未被访问过且与当前节点相连，则递归地进行DFS\n",
    "                dfs(neighbor, component, visited)\n",
    "\n",
    "    # 初始化变量\n",
    "    visited = [False] * len(search_info_list)\n",
    "    components = []\n",
    "\n",
    "    # 对每个节点执行DFS，如果该节点尚未被访问\n",
    "    for i in range(len(search_info_list)):\n",
    "        if not visited[i]:\n",
    "            # 创建一个新的组件列表\n",
    "            current_component = []\n",
    "            # 执行DFS并填充当前组件\n",
    "            dfs(i, current_component, visited)\n",
    "            # 将当前组件添加到结果列表\n",
    "            components.append(current_component)\n",
    "\n",
    "    # 剔除掉只有一个region的component\n",
    "    components = [component for component in components if len(component) > 1]\n",
    "\n",
    "    return components\n",
    "\n",
    "def judge_if_connected(info1, info2):\n",
    "    # 判断两个region是否相邻,先判断name是否相同，再判断level是否相同，再判断左上角坐标的xy加上或者减去wh是否与另一个region的xy相同\n",
    "    if info1[\"name\"] != info2[\"name\"]:\n",
    "        return False\n",
    "    if info1[\"level\"] != info2[\"level\"]:\n",
    "        return False\n",
    "    if int(info1[\"x\"]) + int(info1[\"w\"]) == int(info2[\"x\"]) and int(info1[\"y\"]) == int(info2[\"y\"]):\n",
    "        return True\n",
    "    if int(info1[\"x\"]) - int(info1[\"w\"]) == int(info2[\"x\"]) and int(info1[\"y\"]) == int(info2[\"y\"]):\n",
    "        return True\n",
    "    if int(info1[\"y\"]) + int(info1[\"h\"]) == int(info2[\"y\"]) and int(info1[\"x\"]) == int(info2[\"x\"]):\n",
    "        return True\n",
    "    if int(info1[\"y\"]) - int(info1[\"h\"]) == int(info2[\"y\"]) and int(info1[\"x\"]) == int(info2[\"x\"]):\n",
    "        return True\n",
    "    if int(info1[\"x\"]) + int(info1[\"w\"]) == int(info2[\"x\"]) and int(info1[\"y\"]) + int(info1[\"h\"]) == int(info2[\"y\"]):\n",
    "        return True\n",
    "    if int(info1[\"x\"]) - int(info1[\"w\"]) == int(info2[\"x\"]) and int(info1[\"y\"]) - int(info1[\"h\"]) == int(info2[\"y\"]):\n",
    "        return True\n",
    "    if int(info1[\"x\"]) + int(info1[\"w\"]) == int(info2[\"x\"]) and int(info1[\"y\"]) - int(info1[\"h\"]) == int(info2[\"y\"]):\n",
    "        return True\n",
    "    if int(info1[\"x\"]) - int(info1[\"w\"]) == int(info2[\"x\"]) and int(info1[\"y\"]) + int(info1[\"h\"]) == int(info2[\"y\"]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def request_image(query_img_path):\n",
    "    if \"http\" in query_img_path:\n",
    "        response = requests.get(query_img_path, verify=False)\n",
    "        query_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        query_image = Image.open(query_img_path).convert(\"RGB\")\n",
    "    return query_image\n",
    "\n",
    "def split_img(query_image, other_info):\n",
    "    imgs = []\n",
    "    # 以无重叠的方式切割图片为多个正方形\n",
    "    w, h = query_image.size\n",
    "    if w == h:\n",
    "        return [query_image]\n",
    "    if w > h:\n",
    "        new_w = h\n",
    "        new_h = h\n",
    "    else:\n",
    "        new_w = w\n",
    "        new_h = w\n",
    "    for i in range(0, w, new_w):\n",
    "        for j in range(0, h, new_h):\n",
    "            box = (i, j, i + new_w, j + new_h)\n",
    "            img = query_image.crop(box)\n",
    "            imgs.append(img)\n",
    "    return imgs\n",
    "\n",
    "def get_img_other_info(query_path):\n",
    "    pass\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_index_file(index_file):\n",
    "        index = hnsw.load(index_file, 1024, np.float32, \"sqeuclidean\")\n",
    "        return index\n",
    "\n",
    "def load_info_file(info_file):\n",
    "    with open (info_file, 'r') as f:\n",
    "        infos = json.load(f)\n",
    "    return infos\n",
    "\n",
    "\n",
    "\n",
    "def search(index, encoder, infos_list, query_path, top_k=20):\n",
    "    total_neighbors = []\n",
    "    total_distances = []\n",
    "    query_image = request_image(query_path)\n",
    "    # 根据图片比例尺寸，切割图片\n",
    "    img_other_info = get_img_other_info(query_path)\n",
    "    split_imgs = split_img(query_image, img_other_info)\n",
    "    # 编码后的向量存储到一个二维 np.array 中\n",
    "    split_embeddings = cp.array([cp.array(encoder.encode_image(after_split_img)).astype('float32') for after_split_img in split_imgs])\n",
    "    \n",
    "    # 确保每个 query_embedding 是二维数组\n",
    "    if split_embeddings.ndim == 1:\n",
    "        split_embeddings = split_embeddings.reshape(1, -1)\n",
    "    \n",
    "    # print(\"split_embeddings:\", split_embeddings)\n",
    "    # return split_embeddings\n",
    "    result_infos = []\n",
    "    # search_params = hnsw.SearchParams()\n",
    "    \n",
    "    time_cost = 0\n",
    "    begin_time = time.time()\n",
    "    # distances, neighbors = hnsw.search(search_params, index, split_embeddings, top_k)\n",
    "    distances, neighbors = cagra.search(cagra.SearchParams(),index, split_embeddings, top_k)\n",
    "    end_time = time.time()\n",
    "    time_cost += end_time - begin_time\n",
    "    \n",
    "    neighbors = cp.asarray(neighbors).flatten().tolist()\n",
    "    distances = cp.asarray(distances).flatten().tolist()\n",
    "    for neighbor in neighbors:\n",
    "        result_infos.append(infos_list[neighbor])\n",
    "    total_neighbors.extend(neighbors)\n",
    "    total_distances.extend(distances)\n",
    "\n",
    "    total_distances, total_neighbors, result_infos = zip(*sorted(zip(total_distances, total_neighbors, result_infos)))\n",
    "    return total_distances[:], total_neighbors[:], result_infos[:]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "query_img_path = \" metaservice/api/region/openslide/241183-21.tiff/6400/25344/512/256/1\"\n",
    "image_encoder = WSI_Image_UNI_Encoder()\n",
    "\n",
    "ssd_dir = \"/hpc2ssd/JH_DATA/spooler/ysi538/\"\n",
    "\n",
    "hnsw_index = load_index_file(ssd_dir + \"cupy_index_batch_0.bin\")\n",
    "infos_list = load_info_file(ssd_dir + \"cupy_infos_batch_0.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] [15:53:42.278367] optimizing graph\n",
      "[I] [15:54:07.391248] Graph optimized, creating index\n"
     ]
    }
   ],
   "source": [
    "embeddings = cp.load(ssd_dir + \"cupy_embeddings_batch_0.npy\")\n",
    "# 转换为numpy\n",
    "embeddings = cp.asnumpy(embeddings).astype(np.float32)\n",
    "embeddings = cp.array(embeddings, dtype=cp.float32)\n",
    "build_params = cagra.IndexParams(metric=\"sqeuclidean\",build_algo = 'nn_descent')\n",
    "cuda_index = cagra.build(build_params, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc2hdd/home/ysi538/miniconda3/envs/yhre-new/lib/python3.10/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'mdi.hkust-gz.edu.cn'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "total_distances, total_neighbors, result_infos = search(cuda_index, image_encoder, infos_list, query_img_path, 20)\n",
    "\n",
    "\n",
    "\n",
    "combined_regions = get_combined_regions(result_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'id': '6947',\n",
       "   'name': '2113040-79_无淋巴结转移.tiff',\n",
       "   'x': '22272',\n",
       "   'y': '3584',\n",
       "   'w': '256',\n",
       "   'h': '256',\n",
       "   'level': '1'},\n",
       "  {'id': '6947',\n",
       "   'name': '2113040-79_无淋巴结转移.tiff',\n",
       "   'x': '22272',\n",
       "   'y': '3328',\n",
       "   'w': '256',\n",
       "   'h': '256',\n",
       "   'level': '1'}],\n",
       " [{'id': '47062',\n",
       "   'name': 'EGFR-肺癌-230214010TFXA-0.7-LBP.ibl.tiff',\n",
       "   'x': '9984',\n",
       "   'y': '16640',\n",
       "   'w': '256',\n",
       "   'h': '256',\n",
       "   'level': '1'},\n",
       "  {'id': '47062',\n",
       "   'name': 'EGFR-肺癌-230214010TFXA-0.7-LBP.ibl.tiff',\n",
       "   'x': '10240',\n",
       "   'y': '16896',\n",
       "   'w': '256',\n",
       "   'h': '256',\n",
       "   'level': '1'}],\n",
       " [{'id': '7505',\n",
       "   'name': 'D23-00779-21_无淋巴结转移.tiff',\n",
       "   'x': '11776',\n",
       "   'y': '9472',\n",
       "   'w': '256',\n",
       "   'h': '256',\n",
       "   'level': '1'},\n",
       "  {'id': '7505',\n",
       "   'name': 'D23-00779-21_无淋巴结转移.tiff',\n",
       "   'x': '12032',\n",
       "   'y': '9472',\n",
       "   'w': '256',\n",
       "   'h': '256',\n",
       "   'level': '1'}]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total_distances, total_neighbors, result_infos \n",
    "combined_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 问题1:切分时粒度如何确定？\n",
    "# 问题2:切分后检索结果是否考虑原始位置关系？\n",
    "\n",
    "\n",
    "\n",
    "# 我们的方法流程大致如下：\n",
    "# 1.获得一张病理图片\n",
    "# 2.根据某种切分方式切分图片（可创新）\n",
    "# 3.将切分后的图片分别进行编码，使用Uni\n",
    "# 4.将每个编码进行向量检索，通过batch的方式进行检索（可创新，使用GPU进行加速）\n",
    "# 5.将检索结果通过某种方式进行合并，得到region的检索结果（可创新，使用GPU进行加速，带相对位置信息的dfs+多数投票，可强调权重以展示通用性和框架性）\n",
    "\n",
    "\n",
    "\n",
    "# 问题1:切分时粒度如何确定？\n",
    "# 问题2:检索时如何加速检索？\n",
    "# 问题3:如何将检索结果合并为region的检索结果？\n",
    "\n",
    "# 对比指标：\n",
    "# 1.检索速度和精度的曲线（time-recall)，和传统cpu方法进行对比，和对gpu检索方法进行对比\n",
    "# 2.和单个patch的检索结果对比相似度，以及部分示例（考虑缩略图的特殊情况？）\n",
    "\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yhre-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
